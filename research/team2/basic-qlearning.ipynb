{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "import gym_line_follower  # to register environment\n",
    "\n",
    "# Creating enviroment\n",
    "env = gym.make(\"LineFollower-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paremeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "x_step = 0.025 # Jump between discretized x states\n",
    "y_step = 0.025 # Jump between discretized y states\n",
    "rotor_step = 0.25 # Jump between discretized rotor values\n",
    "\n",
    "\n",
    "epsilon = 0.1 # Chance of exploration\n",
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.6 # Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_rotor =  np.arange(0, 1 + rotor_step, rotor_step) # Possible rotor actions\n",
    "states_x = np.arange(0, 0.3 + x_step, x_step) # Possible x states\n",
    "states_y = np.arange(-0.2, 0.2 + y_step, y_step) # Possible y states\n",
    "\n",
    "# Create a list of tuples, with every possible state\n",
    "states = [(x, y) for y in states_y for x in states_x]\n",
    "# Create a list of tuples, with every possible action\n",
    "actions = [(rotor_one_state, rotor_two_state) for rotor_one_state in states_rotor for rotor_two_state in states_rotor]\n",
    "\n",
    "num_actions = len(actions) # Number of rotor actions\n",
    "num_states = len(states) # Number of states\n",
    "\n",
    "Q = np.zeros((num_states, num_actions)) # Q table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates an index for random action  \n",
    "def random_action(num_actions):\n",
    "    action = np.random.randint(0, num_actions)\n",
    "    return action\n",
    "\n",
    "# Turns given x and y into their index in the state list\n",
    "def x_y_to_state_idx(x, y):\n",
    "    x_idx = -1\n",
    "    for i in np.arange(0, 0.3 + x_step, x_step):\n",
    "        if i > x:\n",
    "            break\n",
    "        x_idx += 1\n",
    "    \n",
    "    y_idx = -1\n",
    "    for i in np.arange(-0.2, 0.2 + y_step, y_step):\n",
    "        if i > y:\n",
    "            break\n",
    "        y_idx += 1\n",
    "    return states.index((states_x[x_idx], states_y[y_idx]))\n",
    "    \n",
    "# Tranforms observation from enviroment into (x,y) state\n",
    "def observation_to_state(obs):\n",
    "    x, y = obs[0], obs[1]\n",
    "    return x_y_to_state_idx(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "- training progress information (reward, time etc)\n",
    "- agent evalutaion\n",
    "'''\n",
    "def train(env, Q=Q, episodes=episodes, alpha=alpha, gamma=gamma, epsilon=epsilon, checkpoint_name='q_table'):\n",
    "    for i in range(episodes):\n",
    "        obs = env.reset()\n",
    "        state = observation_to_state(obs)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                '''\n",
    "                Exploration: doing random action\n",
    "                '''\n",
    "                action_idx = random_action(num_actions)\n",
    "            else:\n",
    "                '''\n",
    "                Exploitation: doing the best action\n",
    "                '''\n",
    "                action_idx = np.argmax(Q[state])\n",
    "        \n",
    "            action = actions[action_idx]\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_state = observation_to_state(next_obs)\n",
    "\n",
    "            # Updating Q-table\n",
    "            old_value = Q[state, action_idx]\n",
    "            next_max = np.max(Q[next_state])\n",
    "            \n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            Q[state, action_idx] = new_value\n",
    "\n",
    "            state = next_state\n",
    "    checkpoint_name += '{}'.format(episodes)\n",
    "    \n",
    "    with open('checkpoint_name', 'wb') as file:\n",
    "        pickle.dump(Q, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-line-env",
   "language": "python",
   "name": "gym-line-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
