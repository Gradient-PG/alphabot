{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "import gym_line_follower  # to register environment\n",
    "\n",
    "# Creating enviroment\n",
    "env = gym.make(\"LineFollower-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paremeters\n",
    "x_step, y_step, rotor_step are defining number of states and size of a Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_step = 0.025 # Jump between discretized x states\n",
    "y_step = 0.025 # Jump between discretized y states\n",
    "rotor_step = 0.25 # Jump between discretized rotor values\n",
    "# Q-learning parameters\n",
    "episodes = 10\n",
    "epsilon = 0.1 # Chance of exploration\n",
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.6 # Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations\n",
    "\n",
    "We define lists containing possible state values and possible action values. We do it so we will be able to discretize our observation space and rotor values into state space and action space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_rotor =  np.arange(0, 1 + rotor_step, rotor_step) # Possible rotor actions\n",
    "# Create a list of tuples, with every possible state\n",
    "states_x = np.arange(0, 0.3 + x_step, x_step) # Possible x states\n",
    "states_y = np.arange(-0.2, 0.2 + y_step, y_step) # Possible y states\n",
    "states = [(x, y) for y in states_y for x in states_x]\n",
    "\n",
    "# Create a list of tuples, with every possible action\n",
    "actions = [(rotor_one_state, rotor_two_state) for rotor_one_state in states_rotor for rotor_two_state in states_rotor]\n",
    "num_actions = len(actions) # Number of rotor actions\n",
    "num_states = len(states) # Number of states\n",
    "\n",
    "Q = np.zeros((num_states, num_actions)) # Q table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "Our input are 8 points which represents a postion of the line. As shown on image below.\n",
    "![line_represntation](img/line_representation.png \"Line represntation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates an index for random action  \n",
    "def random_action(num_actions):\n",
    "    action = np.random.randint(0, num_actions)\n",
    "    return action\n",
    "\n",
    "# Turns given x and y into their index in the state list\n",
    "def x_y_to_state_idx(x, y):\n",
    "    x_idx = -1\n",
    "    for i in np.arange(0, 0.3 + x_step, x_step):\n",
    "        if i > x:\n",
    "            break\n",
    "        x_idx += 1\n",
    "    \n",
    "    y_idx = -1\n",
    "    for i in np.arange(-0.2, 0.2 + y_step, y_step):\n",
    "        if i > y:\n",
    "            break\n",
    "        y_idx += 1\n",
    "    return states.index((states_x[x_idx], states_y[y_idx]))\n",
    "    \n",
    "# Tranforms observation from enviroment into (x,y) state\n",
    "def observation_to_state(obs):\n",
    "    x, y = obs[0], obs[1]\n",
    "    return x_y_to_state_idx(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "#### Q-learning algorithm:\n",
    "- Create table Q of size: number of states x number of actions\n",
    "- For every episode do:\n",
    "    - While episode not done\n",
    "        1. With epsilon probability explore (take random action) or with (1-epsilon) probability exploit (take action with highest reward) \n",
    "        2. Perform this action using env.step(action) and get reward.\n",
    "        3. Get observation from enviroment and convert it to new_state.\n",
    "        4. In Q-table update value under (state_index, action_index) address according to formula 1.1\n",
    "        5. Update state to new state. <br><br>\n",
    "Formula *1.1*     \n",
    "![](img/qlearning.svg \"Update cell in Q-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, Q=Q, episodes=episodes, alpha=alpha, gamma=gamma, epsilon=epsilon, checkpoint_name='q_table'):\n",
    "    for i in range(episodes):\n",
    "        obs = env.reset()\n",
    "        state = observation_to_state(obs)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                '''\n",
    "                Exploration: doing random action\n",
    "                '''\n",
    "                action_idx = random_action(num_actions)\n",
    "            else:\n",
    "                '''\n",
    "                Exploitation: doing the best action\n",
    "                '''\n",
    "                action_idx = np.argmax(Q[state])\n",
    "        \n",
    "            \"\"\"\n",
    "            Performing an action.\n",
    "            \"\"\"\n",
    "            action = actions[action_idx]\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_state = observation_to_state(next_obs)\n",
    "\n",
    "            \"\"\"\n",
    "            Updating Q-table.  \n",
    "            \"\"\"\n",
    "            old_value = Q[state, action_idx]\n",
    "            next_max = np.max(Q[next_state])\n",
    "            Q[state, action_idx] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            state = next_state\n",
    "            \n",
    "    checkpoint_name += '_{}'.format(episodes)\n",
    "    \n",
    "    with open(checkpoint_name, 'wb') as file:\n",
    "        pickle.dump(Q, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-line-env",
   "language": "python",
   "name": "gym-line-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
