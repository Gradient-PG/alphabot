{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "import gym_line_follower  # to register environment\n",
    "\n",
    "# Creating enviroment\n",
    "env = gym.make(\"LineFollower-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paremeters\n",
    "x_step, y_step, rotor_step are defining number of states and size of a Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_step = 0.025 # Jump between discretized x states\n",
    "y_step = 0.025 # Jump between discretized y states\n",
    "rotor_step = 0.25 # Jump between discretized rotor value states was 0.1\n",
    "# Q-learning parameters\n",
    "episodes = 1000\n",
    "epsilon = 0.1 # Chance of exploration\n",
    "alpha = 0.1 # Learning rate\n",
    "gamma = 0.6 # Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations\n",
    "\n",
    "We define lists containing possible state values and possible action values. We do it so we will be able to discretize our observation space and rotor values into state space and action space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_rotor =  np.arange(0, 1 + rotor_step, rotor_step) # Possible rotor actions\n",
    "# Create a list of tuples, with every possible state\n",
    "states_x = np.arange(0, 0.3 + x_step, x_step) # Possible x states\n",
    "states_y = np.arange(-0.2, 0.2 + y_step, y_step) # Possible y states\n",
    "states = [(x, y) for y in states_y for x in states_x]\n",
    "# Create a list of tuples, with every possible action\n",
    "actions = [(rotor_one_state, rotor_two_state) for rotor_one_state in states_rotor for rotor_two_state in states_rotor]\n",
    "extreme_actions = [(1.0, 1.0), (0.0, 1.0), (1.0, 0.0), (0.0, 0.0), (0.75, 0.0), (0.0, 0.75)] # Rotor actions which we want to avoid\n",
    "actions = [action for action in actions if action not in extreme_actions]\n",
    "\n",
    "num_actions = len(actions) # Number of actions\n",
    "num_states = len(states) # Number of states\n",
    "\n",
    "Q = np.zeros((num_states, num_actions)) # Q table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "Our input are 8 points which represents a postion of the line. As shown on image below.\n",
    "![line_represntation](img/line_representation.png \"Line represntation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates an index for random action  \n",
    "def random_action(num_actions: int) -> int:\n",
    "    action = np.random.randint(0, num_actions)\n",
    "    return action\n",
    "\n",
    "\n",
    "def x_y_to_state_index(x: float, y: float) -> int:\n",
    "    \"\"\"\n",
    "        Converts x, y into index from list of states. \n",
    "    \"\"\"\n",
    "    x_idx = -1\n",
    "    for i in np.arange(0, 0.3 + x_step, x_step):\n",
    "        if i > x:\n",
    "            break\n",
    "        x_idx += 1\n",
    "    \n",
    "    y_idx = -1\n",
    "    for i in np.arange(-0.2, 0.2 + y_step, y_step):\n",
    "        if i > y:\n",
    "            break\n",
    "        y_idx += 1\n",
    "    \n",
    "    return states.index((states_x[x_idx], states_y[y_idx]))\n",
    "    \n",
    "def observation_to_state(obs):\n",
    "    \"\"\"\n",
    "        Takes first point from our set of 8 poitns which represent position of the line, \n",
    "        then converts it to state.\n",
    "    \"\"\"\n",
    "    x, y = obs[0], obs[1]\n",
    "    return x_y_to_state_index(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "#### Q-learning algorithm:\n",
    "- Create table Q of size: number of states x number of actions\n",
    "- For every episode do:\n",
    "    - While episode not done\n",
    "        1. With epsilon probability explore (take random action) or with (1-epsilon) probability exploit (take action with highest reward) \n",
    "        2. Perform this action using env.step(action) and get reward.\n",
    "        3. Get observation from enviroment and convert it to new_state.\n",
    "        4. In Q-table update value under (state_index, action_index) address according to formula 1.1\n",
    "        5. Update state to new state. <br><br>\n",
    "Formula *1.1*     \n",
    "![](img/qlearning.svg \"Update cell in Q-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, Q=Q, episodes=episodes, alpha=alpha, gamma=gamma, epsilon=epsilon, checkpoint_name='q_table') -> None:\n",
    "    \"\"\"\n",
    "    Body of Q-learning algorithm.\n",
    "    \"\"\"\n",
    "    for i in range(episodes):\n",
    "        obs = env.reset()\n",
    "        state = observation_to_state(obs)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                '''\n",
    "                Exploration: doing random action\n",
    "                '''\n",
    "                action_idx = None # random action index\n",
    "            else:\n",
    "                '''\n",
    "                Exploitation: doing the best action\n",
    "                '''\n",
    "                action_idx = None # index of action with highest reward for a given state\n",
    "            \"\"\"\n",
    "            TO DO: Performing an action.\n",
    "            \"\"\"\n",
    "            action = None\n",
    "            _, _, done, _ = env.step((0,0))\n",
    "            next_state = None\n",
    "            \"\"\"\n",
    "            Updating Q-table.  \n",
    "            \"\"\"\n",
    "            old_cell_value = None\n",
    "            next_max = np.max(None)\n",
    "            # implememnt formula\n",
    "            # Update Q-table cell according to formula 1.1\n",
    "            state = next_state\n",
    "    checkpoint_name += '{}'.format(episodes)\n",
    "    \n",
    "    with open('checkpoint_name', 'wb') as file:\n",
    "        pickle.dump(Q, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
